{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from diff_pool6_max import DiffPool\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader, DenseDataLoader as DenseLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "import torch_geometric.transforms as T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(r'./data/patient_gumbel_train.pickle', 'rb') as handle:\n",
    "    patient_dict_train = pickle.load(handle)\n",
    "with open(r'./data/patient_gumbel_val.pickle', 'rb') as handle:\n",
    "    patient_dict_val = pickle.load(handle)\n",
    "    \n",
    "patient_dict = defaultdict(list)\n",
    "for dic in (patient_dict_train, patient_dict_val):\n",
    "    for key, value in dic.items():\n",
    "        patient_dict[key] += value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(PatientDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['patient.dataset']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        \n",
    "        data_list = []\n",
    "        node_labels_dict = {'Tumor': 0, 'Stroma': 1, 'TIL1': 2, 'TIL2': 3, 'NK': 4, 'MP': 5}\n",
    "        class_num = len(node_labels_dict)\n",
    "        \n",
    "        for idx, v in enumerate(patient_dict.values()):\n",
    "            for G in v:\n",
    "                node_features = torch.LongTensor([node_labels_dict[i] for i in \n",
    "                                list(nx.get_node_attributes(G, 'cell_types').values())]).unsqueeze(1)\n",
    "                x = torch.zeros(len(G.nodes), class_num).scatter_(1, node_features, 1)\n",
    "                y = torch.LongTensor([idx])\n",
    "                edges = sorted([e for e in G.edges] + [e[::-1] for e in G.edges])\n",
    "                edge_index = torch.tensor([[e[0] for e in edges],\n",
    "                                           [e[1] for e in edges]], dtype=torch.long)\n",
    "                data = Data(x=x, edge_index=edge_index, y=y)\n",
    "                data_list.append(data)\n",
    "        \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, sparse=False):\n",
    "    \n",
    "    dataset = PatientDataset(path)\n",
    "    if not sparse:\n",
    "        max_num_nodes = 0\n",
    "        for data in dataset:\n",
    "            max_num_nodes = max(data.num_nodes, max_num_nodes)\n",
    "\n",
    "        if dataset.transform is None:\n",
    "            dataset.transform = T.ToDense(max_num_nodes)\n",
    "        else:\n",
    "            dataset.transform = T.Compose(\n",
    "                [dataset.transform, T.ToDense(max_num_nodes)])\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = {} \n",
    "# This dataset includes both training and validation data\n",
    "path = './data/patient_gumbel_val'\n",
    "dataset_dict['gumbel2_5'] = get_dataset(path, sparse=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda: 0' if torch.cuda.is_available() else 'cpu')\n",
    "num_patients=10\n",
    "num_patches = 5\n",
    "\n",
    "def run(dataset, model, epochs, batch_size, lr, lr_decay_factor, lr_decay_step_size,\n",
    "        weight_decay, logger=None, resume=None):\n",
    "    \n",
    "    lines = []\n",
    "    train_indices = []\n",
    "    for i in range(num_patients):\n",
    "        tmp = [2*num_patches*i+ j for j in range(num_patches)]\n",
    "        train_indices += tmp\n",
    "    test_indices = sorted(list(set(range(num_patients*num_patches*2)) - set(train_indices)))\n",
    "    train_indices = torch.tensor(train_indices)\n",
    "    test_indices = torch.tensor(test_indices)\n",
    "    train_dataset = dataset[train_indices]\n",
    "    test_dataset = dataset[test_indices]\n",
    "    \n",
    "    if 'adj' in dataset[0]:\n",
    "        # This data loader only works with dense adjacency matrices\n",
    "        train_loader = DenseLoader(train_dataset, batch_size, shuffle=False)\n",
    "        test_loader = DenseLoader(test_dataset, batch_size, shuffle=False)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "        \n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # save on cpu, load on cpu\n",
    "    if resume:\n",
    "        last_checkpoint = torch.load(dir_path + 'checkpoint_last.pt')\n",
    "        model.load_state_dict(last_checkpoint['state_dict'])\n",
    "        model.to(device)\n",
    "        optimizer.load_state_dict(last_checkpoint['optimizer'])\n",
    "        start_epoch = last_checkpoint['epoch']+1\n",
    "    else:\n",
    "        model.to(device).reset_parameters()\n",
    "        start_epoch = 1\n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "            \n",
    "    # !!! save initial parameters\n",
    "    torch.save(model.state_dict(), dir_path+'params_epoch{}.pt'.format(0))\n",
    "    \n",
    "    t_start = time.perf_counter()\n",
    "    \n",
    "    for epoch in tqdm(range(start_epoch, start_epoch + epochs)):\n",
    "        train_loss, train_acc = train(model, optimizer, train_loader)\n",
    "\n",
    "        eval_info = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "        }\n",
    "\n",
    "        if logger is not None:\n",
    "            lines.append(logger(eval_info))\n",
    "\n",
    "        if epoch % lr_decay_step_size == 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_decay_factor * param_group['lr']\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            test_loss, test_acc = eval_loss_acc(model, test_loader)\n",
    "            lines.append('Test Loss: {:.4f}, Test Accuracy: {:.3f}'.format(test_loss, test_acc))\n",
    "            torch.save(model.state_dict(), dir_path+'params_epoch{}.pt'.format(epoch))\n",
    "            \n",
    "    checkpoint = {'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, dir_path + 'checkpoint_last.pt')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t_end = time.perf_counter()\n",
    "    duration = t_end - t_start\n",
    "    lines.append('Duration: {:.3f}'.format(duration))\n",
    "    \n",
    "    return lines\n",
    "      \n",
    "\n",
    "# def shuffle(dataset, m=20):\n",
    "#     indices = []\n",
    "#     for i in range(10):\n",
    "#         tmp = [j for j in range(i*m, i*m+m)]\n",
    "#         random.shuffle(tmp)\n",
    "#         indices += tmp\n",
    "#     return indices\n",
    "\n",
    "def train(model, optimizer, loader):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        len_ = len(data.y)\n",
    "        indices = [i for i in range(0, len_, num_patches)]\n",
    "        loss = F.nll_loss(out, data.y[indices].view(-1), reduction='sum')\n",
    "        \n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        pred = out.max(1)[1]\n",
    "        correct += pred.eq(data.y[indices].view(-1)).sum().item()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / (len(loader.dataset)/num_patches), correct / (len(loader.dataset)/num_patches)\n",
    "\n",
    "def eval_loss_acc(model, loader):\n",
    "    model.eval()\n",
    "    \n",
    "    y_pred = []\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "        pred = out.max(1)[1]\n",
    "        y_pred += pred.tolist()\n",
    "        len_ = len(data.y)\n",
    "        indices = [i for i in range(0, len_, num_patches)]\n",
    "        correct += pred.eq(data.y[indices].view(-1)).sum().item()\n",
    "        loss += F.nll_loss(out, data.y[indices].view(-1), reduction='sum').item()\n",
    "    return loss / (len(loader.dataset)/num_patches), correct / (len(loader.dataset)/num_patches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe208f0f564409bafe4036611caf739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_layers = 5\n",
    "hidden = 64\n",
    "num_hops = 2\n",
    "batch_size = 50\n",
    "ratio = 0.05\n",
    "dropout = False\n",
    "Net =  DiffPool \n",
    "\n",
    "def logger(info):\n",
    "    epoch = info['epoch']\n",
    "    train_loss, train_acc = info['train_loss'], info['train_acc']\n",
    "    output = '{:03d}: Train Loss: {:.4f}, Train Accuracy: {:.3f}'\\\n",
    "              .format(epoch, train_loss, train_acc)\n",
    "    return output\n",
    "\n",
    "dir_path = './data'+ '/' + 'DiffPool_diff_pool6_max_bs50'+ '/' + 'gumbel2_5' + '/'\n",
    "hyperparams_name = 'hyperparams.pickle'\n",
    "\n",
    "if os.path.exists(dir_path + hyperparams_name):\n",
    "    with open(r'{}'.format(dir_path + hyperparams_name), 'rb') as handle:\n",
    "        hyperparams = pickle.load(handle)\n",
    "        num_layers = hyperparams['# of layers']\n",
    "        hidden = hyperparams['# of hidden units']\n",
    "        num_hops = hyperparams['# of hops']\n",
    "        batch_size = hyperparams['batch size']\n",
    "        ratio = hyperparams['pooling ratio']\n",
    "        dropout = hyperparams['dropout']\n",
    "        num_patches = hyperparams['# of patches']\n",
    "\n",
    "else:\n",
    "    lines = ['-----\\{}'.format(Net.__name__)]\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    lines.append('Num of Layers: {}, Num of Hidden Units: {}, Num of Hops: {}, Batch Size: {}, ' \\\n",
    "                 'Pooling Ratio: {}, Dropout: {}, Num of Patches: {}' \\\n",
    "                 .format(num_layers, hidden, num_hops, batch_size, ratio, dropout, num_patches))\n",
    "    dataset = dataset_dict['gumbel2_5']\n",
    "    # For diff_pool6\n",
    "    model = Net(dataset, num_layers, hidden, hop=num_hops, num_patches=num_patches, ratio=ratio, dropout=dropout)\n",
    "\n",
    "    process_lines = \\\n",
    "    run(\n",
    "        dataset,\n",
    "        model,\n",
    "        epochs=500,\n",
    "        batch_size=batch_size,\n",
    "        lr=0.01,\n",
    "        lr_decay_factor=0.5,\n",
    "        lr_decay_step_size=50,\n",
    "        weight_decay=0,\n",
    "        logger=logger\n",
    "    )\n",
    "    lines += process_lines\n",
    "\n",
    "    hyperparams = {'# of layers': num_layers, '# of hidden units': hidden, '# of hops': num_hops, \\\n",
    "                   'batch size': batch_size, 'pooling ratio': ratio, 'dropout':dropout, '# of patches': num_patches}\n",
    "                           \n",
    "    with open(r'{}'.format(dir_path + hyperparams_name), 'wb') as handle:\n",
    "        pickle.dump(hyperparams, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    filename = 'log_' + date_time + '.txt'\n",
    "    logfile = open(dir_path + filename, 'w')\n",
    "    for line in lines:\n",
    "        logfile.write(\"{}\\n\".format(line))\n",
    "    logfile.close()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geodeep)",
   "language": "python",
   "name": "geodeep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
