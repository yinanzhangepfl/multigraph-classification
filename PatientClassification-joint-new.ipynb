{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from gumbel_softmax import DiffPool\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader, DenseDataLoader as DenseLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "import torch_geometric.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(r'./data/patient_gumbel_train.pickle', 'rb') as handle:\n",
    "    patient_dict_train = pickle.load(handle)\n",
    "with open(r'./data/patient_gumbel_val.pickle', 'rb') as handle:\n",
    "    patient_dict_val = pickle.load(handle)\n",
    "    \n",
    "patient_dict = defaultdict(list)\n",
    "for dic in (patient_dict_train, patient_dict_val):\n",
    "    for key, value in dic.items():\n",
    "        patient_dict[key] += value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(PatientDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['patient.dataset']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        \n",
    "        data_list = []\n",
    "        node_labels_dict = {'Tumor': 0, 'Stroma': 1, 'TIL1': 2, 'TIL2': 3, 'NK': 4, 'MP': 5}\n",
    "#         node_labels_dict = {'TIL':0, 'Tumor': 1, 'Stroma': 2}\n",
    "        class_num = len(node_labels_dict)\n",
    "        \n",
    "        for idx, v in enumerate(patient_dict.values()):\n",
    "            for G in v:\n",
    "                node_features = torch.LongTensor([node_labels_dict[i] for i in \n",
    "                                list(nx.get_node_attributes(G, 'cell_types').values())]).unsqueeze(1)\n",
    "                x = torch.zeros(len(G.nodes), class_num).scatter_(1, node_features, 1)\n",
    "                y = torch.LongTensor([idx])\n",
    "                edges = sorted([e for e in G.edges] + [e[::-1] for e in G.edges])\n",
    "                edge_index = torch.tensor([[e[0] for e in edges],\n",
    "                                           [e[1] for e in edges]], dtype=torch.long)\n",
    "                data = Data(x=x, edge_index=edge_index, y=y)\n",
    "                data_list.append(data)\n",
    "        \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, sparse=False):\n",
    "    \n",
    "    dataset = PatientDataset(path)\n",
    "    if not sparse:\n",
    "        max_num_nodes = 0\n",
    "        for data in dataset:\n",
    "            max_num_nodes = max(data.num_nodes, max_num_nodes)\n",
    "\n",
    "        if dataset.transform is None:\n",
    "            dataset.transform = T.ToDense(max_num_nodes)\n",
    "        else:\n",
    "            dataset.transform = T.Compose(\n",
    "                [dataset.transform, T.ToDense(max_num_nodes)])\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {}    \n",
    "# This dataset includes both training and validation data\n",
    "path = './data/patient_gumbel_val'\n",
    "dataset_dict['gumbel2_5'] = get_dataset(path, sparse=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatientDataset(100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['gumbel2_5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InclassShuffleSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, data_source, m):\n",
    "        self.data_source = data_source\n",
    "        self.m = m\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(shuffle(self.data_source, self.m))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different learning rates for a specific layer  \n",
    "https://discuss.pytorch.org/t/different-learning-rate-for-a-specific-layer/33670/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda: 0' if torch.cuda.is_available() else 'cpu')\n",
    "num_patients = 10\n",
    "num_patches = 2\n",
    "total = 5\n",
    "\n",
    "def run(dataset, model, temp, total, annealing_rate, lower_bound, epochs, batch_size, \\\n",
    "        lr_base, lr, lr_decay_factor, lr_decay_step_size, weight_decay, logger=None, \\\n",
    "        resume=None, aux_loss=False, alpha1=1, alpha2=0, alpha3=0):\n",
    "    \n",
    "    lines = []\n",
    "    train_indices = []\n",
    "    for i in range(num_patients):\n",
    "        tmp = [2*total*i+ j for j in range(total)]\n",
    "        train_indices += tmp\n",
    "    val_indices = sorted(list(set(range(num_patients*total*2)) - set(train_indices)))\n",
    "    train_indices = torch.tensor(train_indices)\n",
    "    val_indices = torch.tensor(val_indices)\n",
    "    train_dataset = dataset[train_indices]\n",
    "    val_dataset = dataset[val_indices]\n",
    "    train_sampler = InclassShuffleSampler(train_dataset, total)\n",
    "    val_sampler = InclassShuffleSampler(val_dataset, total)\n",
    "    \n",
    "    if 'adj' in dataset[0]:\n",
    "        # This data loader only works with dense adjacency matrices\n",
    "        train_loader = DenseLoader(train_dataset, batch_size, shuffle=False, sampler=train_sampler)\n",
    "        val_loader = DenseLoader(val_dataset, batch_size, shuffle=False, sampler=val_sampler)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle=False, sampler=train_sampler)\n",
    "        val_loader = DataLoader(val_dataset, batch_size, shuffle=False, sampler=val_sampler)\n",
    "        \n",
    "    # !!! optimizer  \n",
    "    parameters_list = ['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']\n",
    "    parameters = [kv[1] for kv in model.named_parameters() if kv[0] in parameters_list]\n",
    "    base_parameters = [kv[1] for kv in model.named_parameters() if kv[0] not in parameters_list]\n",
    "    optimizer = Adam([{'params': base_parameters},\n",
    "                      {'params': parameters, 'lr': lr}\n",
    "                     ], lr=lr_base, weight_decay=weight_decay)\n",
    "    \n",
    "    # save on cpu, load on cpu\n",
    "    if resume:\n",
    "        last_checkpoint = torch.load(dir_path + 'checkpoint_last.pt')\n",
    "        model.load_state_dict(last_checkpoint['state_dict'])\n",
    "        model.to(device)\n",
    "        optimizer.load_state_dict(last_checkpoint['optimizer'])\n",
    "        start_epoch = last_checkpoint['epoch']+1\n",
    "    else:\n",
    "        # !!! randomly initialize parameters of the sampling module\n",
    "        model.to(device).reset_gumbel()\n",
    "        # !!! or randomly initialize all parameters if no pre-training\n",
    "#         model.to(device).reset_parameters()\n",
    "        start_epoch = 1\n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "            \n",
    "    # !!! save initial parameters\n",
    "    torch.save(model.state_dict(), dir_path+'params_epoch{}.pt'.format(0))\n",
    "    \n",
    "    t_start = time.perf_counter()\n",
    "    \n",
    "    for epoch in tqdm(range(start_epoch, start_epoch + epochs)):\n",
    "        train_loss, train_acc = train(model, optimizer, train_loader, initial_temp, annealing_rate, \\\n",
    "                                      lower_bound, epoch, total, aux_loss, alpha1, alpha2, alpha3)\n",
    "\n",
    "        eval_info = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "        }\n",
    "\n",
    "        if logger is not None:\n",
    "            lines.append(logger(eval_info))\n",
    "\n",
    "        if epoch % lr_decay_step_size == 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_decay_factor * param_group['lr']\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            val_loss, val_acc = eval_loss_acc(model, val_loader, initial_temp, \\\n",
    "                                              annealing_rate, lower_bound, epoch, total)\n",
    "            lines.append('Val Loss: {:.4f}, Val Accuracy: {:.3f}'.format(val_loss, val_acc))\n",
    "            torch.save(model.state_dict(), dir_path+'params_epoch{}.pt'.format(epoch))\n",
    "            \n",
    "    checkpoint = {'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, dir_path + 'checkpoint_last.pt')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t_end = time.perf_counter()\n",
    "    duration = t_end - t_start\n",
    "    lines.append('Duration: {:.3f}'.format(duration))\n",
    "    \n",
    "    return lines\n",
    "      \n",
    "\n",
    "def shuffle(dataset, m=20):\n",
    "    indices = []\n",
    "    for i in range(10):\n",
    "        tmp = [j for j in range(i*m, i*m+m)]\n",
    "        random.shuffle(tmp)\n",
    "        indices += tmp\n",
    "    return indices\n",
    "\n",
    "\n",
    "def cosine_distance(x1, x2=None, eps=1e-8):\n",
    "    x2 = x1 if x2 is None else x2\n",
    "    w1 = x1.norm(p=2, dim=1, keepdim=True)\n",
    "    w2 = w1 if x2 is x1 else x2.norm(p=2, dim=1, keepdim=True)\n",
    "    tmp = 1 - torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps)\n",
    "    tmp = torch.triu(tmp, diagonal=1)\n",
    "    res = tmp.sum() / ((tmp.size()[0] * tmp.size()[1] - tmp.size()[1]) / 2)\n",
    "    return res\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader, initial_temp, annealing_rate, lower_bound, epoch, \\\n",
    "          total=10, aux_loss=False, alpha1=1, alpha2=0, alpha3=0):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    base = len(loader)*(epoch-1)\n",
    "    num_patients = (len(loader.dataset)/total)\n",
    "    \n",
    "    for batch_idx, data in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        iteration = base + batch_idx\n",
    "        temp = np.maximum(initial_temp * np.exp(-annealing_rate * iteration), lower_bound)\n",
    "        if aux_loss:\n",
    "            out, discard_graphs, z = model(data, temp)\n",
    "        else:\n",
    "            out = model(data, temp)\n",
    "        len_ = len(data.y)\n",
    "        indices = [i for i in range(0, len_, total)]\n",
    "        # !!! loss function\n",
    "        if aux_loss:\n",
    "            loss = alpha1*F.nll_loss(out, data.y[indices].view(-1), reduction='sum') / num_patients + \\\n",
    "                   alpha2*F.pdist(discard_graphs, p=2).mean() / len(loader) + \\\n",
    "                   alpha3*(z[:, 0, :]*z[:, 1, :]).sum() / num_patients\n",
    "#             loss = alpha1*F.nll_loss(out, data.y[indices].view(-1), reduction='sum') / num_patients + \\\n",
    "#                    alpha2*cosine_distance(discard_graphs).mean() / len(loader)\n",
    "        else:\n",
    "            loss = F.nll_loss(out, data.y[indices].view(-1), reduction='sum') / num_patients\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        pred = out.max(1)[1]\n",
    "        correct += pred.eq(data.y[indices].view(-1)).sum().item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss, correct / num_patients\n",
    "\n",
    "\n",
    "def eval_loss_acc(model, loader, initial_temp, annealing_rate, lower_bound, epoch, total=10):\n",
    "    model.eval()\n",
    "    \n",
    "    y_pred = []\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    iteration = len(loader)*(epoch-1)\n",
    "    temp = np.maximum(initial_temp * np.exp(-annealing_rate * iteration), lower_bound)\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data, temp)\n",
    "        pred = out.max(1)[1]\n",
    "        y_pred += pred.tolist()\n",
    "        len_ = len(data.y)\n",
    "        indices = [i for i in range(0, len_, total)]\n",
    "        correct += pred.eq(data.y[indices].view(-1)).sum().item()\n",
    "        loss += F.nll_loss(out, data.y[indices].view(-1), reduction='sum').item()\n",
    "    return loss / (len(loader.dataset)/total), correct / (len(loader.dataset)/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# temparature annealing schedule \n",
    "x = [i for i in range(1, 501)]\n",
    "y = [np.maximum(10 * np.exp(-0.01 * i), 0.5) for i in x]\n",
    "plt.plot(x, y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd2ed4092084af48e6f06a7431dd7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## %%capture cap --no-stderr\n",
    "num_layers = 5\n",
    "hidden = 64\n",
    "num_hops = 2\n",
    "batch_size = 50\n",
    "ratio = 0.05\n",
    "dropout = False\n",
    "Net =  DiffPool \n",
    "initial_temp = 10\n",
    "annealing_rate = 0.01\n",
    "lower_bound = 0.5\n",
    "hard = True\n",
    "aux_loss = False\n",
    "agg = 'max'\n",
    "alpha1 = 1\n",
    "alpha2 = 0\n",
    "alpha3 = 0\n",
    "lr_base = 1e-6\n",
    "lr = 1e-4\n",
    "decay_rate = 1\n",
    "\n",
    "def logger(info):\n",
    "    epoch = info['epoch']\n",
    "    train_loss, train_acc = info['train_loss'], info['train_acc']\n",
    "    output = '{:03d}: Train Loss: {:.4f}, Train Accuracy: {:.3f}'\\\n",
    "              .format(epoch, train_loss, train_acc)\n",
    "    return output\n",
    "\n",
    "dir_path = './data'+ '/' + 'DiffPool_gs_bs50_r005_joint'+ '/' + 'gumbel2_5-max-hard-max' + '/'\n",
    "# dir_path = './data' + '/' + 'DiffPool_gs_bs50_r005_joint'+ '/' + 'gumbel1_5-avg-hard' + '/'\n",
    "hyperparams_name = 'hyperparams.pickle'\n",
    "\n",
    "\n",
    "if os.path.exists(dir_path + hyperparams_name):\n",
    "    with open(r'{}'.format(dir_path + hyperparams_name), 'rb') as handle:\n",
    "        hyperparams = pickle.load(handle)\n",
    "        num_layers = hyperparams['# of layers']\n",
    "        hidden = hyperparams['# of hidden units']\n",
    "        num_hops = hyperparams['# of hops']\n",
    "        batch_size = hyperparams['batch size']\n",
    "        ratio = hyperparams['pooling ratio']\n",
    "        dropout = hyperparams['dropout']\n",
    "        num_patches = hyperparams['# of patches']\n",
    "\n",
    "\n",
    "else:\n",
    "    lines = ['-----\\{}'.format(Net.__name__)]\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    lines.append('Num of Layers: {}, Num of Hidden Units: {}, Num of Hops: {}, Batch Size: {}, ' \\\n",
    "                 'Pooling Ratio: {}, Dropout: {}, Num of Patches: {}, Total Patches: {}, ' \\\n",
    "                 'Initial temperature: {}, Annealing Rate: {}, Lower Bound: {}, ' \\\n",
    "                 'Auxiliary Loss: {}, Alpha1: {}, Alpha2: {}, Alpha3: {}, ' \\\n",
    "                 'Base Learning Rate: {}, Learning Rate: {}, Decay Rate: {}, hard:{}'\n",
    "                 .format(num_layers, hidden, num_hops, batch_size, ratio, dropout, num_patches, total, \\\n",
    "                         initial_temp, annealing_rate, lower_bound, aux_loss, \\\n",
    "                         alpha1, alpha2, alpha3, lr_base, lr, decay_rate, hard))\n",
    "    dataset = dataset_dict['gumbel2_5']\n",
    "    model = Net(dataset, num_layers, hidden, hop=num_hops, num_patches=num_patches, ratio=ratio, dropout=dropout, \\\n",
    "            total=total, hard_train=hard, aux_loss=aux_loss, agg=agg, decay_rate=decay_rate)\n",
    "    # !!! load pre-trainined parameters\n",
    "    orig_dir_path = './data/DiffPool_diff_pool6_max_bs50/gumbel2_5/'\n",
    "    params_name = 'params_epoch200.pt'\n",
    "    state = model.state_dict()\n",
    "    params = torch.load(orig_dir_path+params_name, map_location=device)\n",
    "    state.update(params)\n",
    "    model.load_state_dict(state)\n",
    "    \n",
    "    process_lines = \\\n",
    "    run(\n",
    "        dataset,\n",
    "        model,\n",
    "        initial_temp,\n",
    "        total,\n",
    "        annealing_rate,\n",
    "        lower_bound,\n",
    "        epochs=500,\n",
    "        batch_size=batch_size,\n",
    "        lr_base=lr_base,\n",
    "        lr=lr,\n",
    "        lr_decay_factor=0.5,\n",
    "        lr_decay_step_size=50,\n",
    "        weight_decay=0,\n",
    "        logger=logger,\n",
    "        aux_loss=aux_loss,\n",
    "        alpha1=alpha1,\n",
    "        alpha2=alpha2,\n",
    "        alpha3=alpha3\n",
    "    )\n",
    "    lines += process_lines\n",
    "\n",
    "    hyperparams = {'# of layers': num_layers, '# of hidden units': hidden, '# of hops': num_hops, \\\n",
    "                   'batch size': batch_size, 'pooling ratio': ratio, 'dropout':dropout, '# of patches': num_patches}\n",
    "\n",
    "    with open(r'{}'.format(dir_path + hyperparams_name), 'wb') as handle:\n",
    "        pickle.dump(hyperparams, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    filename = 'log_' + date_time + '.txt'\n",
    "    logfile = open(dir_path + filename, 'w')\n",
    "    for line in lines:\n",
    "        logfile.write(\"{}\\n\".format(line))\n",
    "    logfile.close()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geodeep)",
   "language": "python",
   "name": "geodeep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
